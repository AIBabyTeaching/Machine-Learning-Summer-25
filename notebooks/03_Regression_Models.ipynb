{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278b5b1d",
   "metadata": {},
   "source": [
    "# Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4819ae21",
   "metadata": {},
   "source": [
    "Bias--variance tradeoff is key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaad348",
   "metadata": {},
   "source": [
    "### Regression Pipeline Overview\n",
    "```text\n",
    "Data -> [Train/Test Split] -> [Train Model] -> [Predict] -> [Evaluate]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linregformula",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "We model the target as $$y = Xw + b$$ and minimize the mean squared error:\n",
    "$$\text{MSE}=\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569a0292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T17:53:59.796139Z",
     "iopub.status.busy": "2025-07-19T17:53:59.795951Z",
     "iopub.status.idle": "2025-07-19T17:54:00.984969Z",
     "shell.execute_reply": "2025-07-19T17:54:00.983173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999996298908006"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = pd.read_csv('../data/house_prices.csv')\n",
    "y = X.pop('SalePrice')\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n",
    "model=LinearRegression().fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f17746",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Adds L2 penalty to the loss function:\n",
    "$$\\hat{w}=\\arg\\min_w \\|y-Xw\\|^2+\\alpha \\|w\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abe620c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999765281399"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge=Ridge(alpha=1.0).fit(X_train,y_train)\n",
    "ridge.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93dd139",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Uses an L1 penalty to encourage sparsity:\n",
    "$$\\hat{w}=\\arg\\min_w \\|y-Xw\\|^2+\\alpha \\|w\\|_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c16801b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999962838680163"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso=Lasso(alpha=0.1).fit(X_train,y_train)\n",
    "lasso.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ba979",
   "metadata": {},
   "source": [
    "### L1 vs L2 Regularization\n",
    "\n",
    "L1 (lasso) adds the absolute value of coefficients to the loss, encouraging sparsity by driving some weights to exactly zero. L2 (ridge) adds the squared magnitude of coefficients, shrinking them toward zero but rarely eliminating features entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7acebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_preds = ridge.predict(X_test.iloc[:3])\n",
    "lasso_preds = lasso.predict(X_test.iloc[:3])\n",
    "pd.DataFrame({'Ridge': ridge_preds, 'Lasso': lasso_preds})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a62406",
   "metadata": {},
   "source": [
    "### Lasso as Feature Eliminator\n",
    "Lasso can automatically drop irrelevant features by forcing their coefficients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_series = pd.Series(lasso.coef_, index=X_train.columns)\n",
    "eliminated_features = list(coef_series[coef_series == 0].index)\n",
    "eliminated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c7f61",
   "metadata": {},
   "source": [
    "### ElasticNet Regression\n",
    "Combines L1 and L2 penalties to balance between Ridge and Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=0).fit(X_train, y_train)\n",
    "elastic.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641cd052",
   "metadata": {},
   "source": [
    "### Random Forest Regression\n",
    "An ensemble of decision trees that reduces variance by averaging multiple trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=0).fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65a479",
   "metadata": {},
   "source": [
    "### Regularization Strength and Model Performance\n",
    "To illustrate how adjusting the regularization penalty impacts performance, let's explore Ridge and Lasso regression on the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "alphas = np.logspace(-3, 3, 7)\n",
    "ridge_train, ridge_test, lasso_nonzero = [], [], []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha).fit(X_train_scaled, y_train)\n",
    "    ridge_train.append(ridge.score(X_train_scaled, y_train))\n",
    "    ridge_test.append(ridge.score(X_test_scaled, y_test))\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000).fit(X_train_scaled, y_train)\n",
    "    lasso_nonzero.append(np.sum(lasso.coef_ != 0))\n",
    "\n",
    "plt.semilogx(alphas, ridge_train, label='Train $R^2$')\n",
    "plt.semilogx(alphas, ridge_test, label='Test $R^2$')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.title('Ridge Regression Performance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.semilogx(alphas, lasso_nonzero, marker='o')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Non-zero coefficients')\n",
    "plt.title('Lasso Feature Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c63f180",
   "metadata": {},
   "source": [
    "Increasing the penalty (`alpha`) shrinks coefficients. In Ridge, large `alpha` reduces both training and test scores due to high bias, while too small `alpha` can overfit. Lasso drives more coefficients to zero as `alpha` grows, highlighting its feature selection behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proscons",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages\n",
    "**Advantages**:\n",
    "- Simple to interpret coefficients.\n",
    "- Fast to train.\n",
    "**Disadvantages**:\n",
    "- Assumes linear relationships.\n",
    "- Sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf446a",
   "metadata": {},
   "source": [
    "### Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8871b42f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T17:54:01.069466Z",
     "iopub.status.busy": "2025-07-19T17:54:01.049018Z",
     "iopub.status.idle": "2025-07-19T17:54:01.149882Z",
     "shell.execute_reply": "2025-07-19T17:54:01.146305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([202.22093264, -12.19762234])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test.iloc[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90beb02e",
   "metadata": {},
   "source": [
    "### Results and Interpretation\n",
    "The prediction snippet above demonstrates how the trained regression model outputs continuous values for new samples. Interpreting coefficients helps identify feature influence, while residual analysis reveals model fit quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32684d90-8ef7-4b57-9836-9730283aa0b0",
   "metadata": {},
   "source": [
    "### Linear Regression Refresher\n",
    "\n",
    "Linear regression models a target variable $y$ as a linear combination of features $x_1, \\ldots, x_p$:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p$$\n",
    "\n",
    "The optimal coefficients minimize the residual sum of squares:\n",
    "\n",
    "$$\\min_\\beta \\|y - X\\beta\\|^2,$$\n",
    "\n",
    "which has the closed-form solution (normal equation):\n",
    "\n",
    "$$\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec2055-d124-4b3b-b409-22d2992f1559",
   "metadata": {},
   "source": [
    "### Worked Example: Interpreting Coefficients\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.array([[800], [1000], [1200]])  # square footage\n",
    "y = np.array([150_000, 200_000, 230_000])\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "print(model.intercept_)\n",
    "print(model.coef_)\n",
    "print(model.predict([[900]]))\n",
    "```\n",
    "\n",
    "The intercept represents the predicted price for a home with zero square feet, and the coefficient indicates the expected change in price for each additional square foot. The final line predicts the price for a 900 square-foot home.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c00ff9",
   "metadata": {},
   "source": [
    "### Exercises & Further Reading\n",
    "1. Add Ridge/Lasso.\n",
    "2. Perform cross validation.\n",
    "3. [sklearn linear models](https://scikit-learn.org/stable/modules/linear_model.html)\n",
    "4. Plot residuals for a fitted model and comment on any patterns.\n",
    "5. Use `PolynomialFeatures` to fit a quadratic curve and compare it to the linear fit.\n",
    "6. Derive the normal equation yourself and verify the solution using NumPy.\n",
    "7. Implement `ElasticNet` and compare its performance to Ridge and Lasso.\n",
    "8. Evaluate the model using mean absolute error and compare it with mean squared error.\n",
    "9. Use `Lasso` to eliminate features and retrain a linear regression model.\n",
    "10. Train a `RandomForestRegressor` and inspect its feature importances.\n",
    "11. Fit an `SVR` model and compare its predictions to linear regression.\n",
    "12. Investigate how varying `alpha` affects `Ridge` and `Lasso` coefficient magnitudes.\n",
    "13. Try scaling features before regularization and observe the impact on coefficients.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitocluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
