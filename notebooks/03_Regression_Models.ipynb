{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278b5b1d",
   "metadata": {},
   "source": [
    "# Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4819ae21",
   "metadata": {},
   "source": [
    "Bias--variance tradeoff is key. [wiki](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linregformula",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "We model the target as $$y = Xw + b$$ and minimize the mean squared error:\n",
    "$$\text{MSE}=\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "569a0292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T17:53:59.796139Z",
     "iopub.status.busy": "2025-07-19T17:53:59.795951Z",
     "iopub.status.idle": "2025-07-19T17:54:00.984969Z",
     "shell.execute_reply": "2025-07-19T17:54:00.983173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999996298908006"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = pd.read_csv('../data/house_prices.csv')\n",
    "y = X.pop('SalePrice')\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n",
    "model=LinearRegression().fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Adds L2 penalty to the loss function:\n",
    "$$\\hat{w}=\\arg\\min_w \\|y-Xw\\|^2+\\alpha \\|w\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge=Ridge(alpha=1.0).fit(X_train,y_train)\n",
    "ridge.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Uses an L1 penalty to encourage sparsity:\n",
    "$$\\hat{w}=\\arg\\min_w \\|y-Xw\\|^2+\\alpha \\|w\\|_1$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso=Lasso(alpha=0.1).fit(X_train,y_train)\n",
    "lasso.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proscons",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages\n",
    "**Advantages**:\n",
    "- Simple to interpret coefficients.\n",
    "- Fast to train.\n",
    "**Disadvantages**:\n",
    "- Assumes linear relationships.\n",
    "- Sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf446a",
   "metadata": {},
   "source": [
    "### Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8871b42f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T17:54:01.069466Z",
     "iopub.status.busy": "2025-07-19T17:54:01.049018Z",
     "iopub.status.idle": "2025-07-19T17:54:01.149882Z",
     "shell.execute_reply": "2025-07-19T17:54:01.146305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([202.22093264, -12.19762234])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test.iloc[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Refresher\n",
    "\n",
    "Linear regression models a target variable $y$ as a linear combination of features $x_1, \\ldots, x_p$:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p$$\n",
    "\n",
    "The optimal coefficients minimize the residual sum of squares:\n",
    "\n",
    "$$\\min_\\beta \\|y - X\\beta\\|^2,$$\n",
    "\n",
    "which has the closed-form solution (normal equation):\n",
    "\n",
    "$$\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y.$$\n"
   ],
   "id": "32684d90-8ef7-4b57-9836-9730283aa0b0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example: Interpreting Coefficients\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.array([[800], [1000], [1200]])  # square footage\n",
    "y = np.array([150_000, 200_000, 230_000])\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "print(model.intercept_)\n",
    "print(model.coef_)\n",
    "print(model.predict([[900]]))\n",
    "```\n",
    "\n",
    "The intercept represents the predicted price for a home with zero square feet, and the coefficient indicates the expected change in price for each additional square foot. The final line predicts the price for a 900 square-foot home.\n"
   ],
   "id": "c6ec2055-d124-4b3b-b409-22d2992f1559"
  },
  {
   "cell_type": "markdown",
   "id": "48c00ff9",
   "metadata": {},
   "source": [
    "### Exercises & Further Reading\n",
    "1. Add Ridge/Lasso.\n",
    "2. Perform cross validation.\n",
    "3. [sklearn linear models](https://scikit-learn.org/stable/modules/linear_model.html)",
    "4. Plot residuals for a fitted model and comment on any patterns.\n",
    "5. Use `PolynomialFeatures` to fit a quadratic curve and compare it to the linear fit.\n",
    "6. Derive the normal equation yourself and verify the solution using NumPy.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}