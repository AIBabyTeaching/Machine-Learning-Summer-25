{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac8ee88",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "## Week 5: PCA, K-Means, and DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objectives",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand** the difference between supervised and unsupervised learning\n",
    "2. **Apply** Principal Component Analysis (PCA) for dimensionality reduction\n",
    "3. **Implement** K-Means clustering and interpret its results\n",
    "4. **Use** DBSCAN for density-based clustering\n",
    "5. **Compare** different clustering algorithms and choose the appropriate one\n",
    "6. **Evaluate** clustering quality using various metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## 1. Introduction to Unsupervised Learning\n",
    "\n",
    "**Unsupervised learning** is a type of machine learning where we work with **unlabeled data**. Unlike supervised learning, there are no target variables to predict. Instead, we aim to discover hidden patterns, structures, or relationships in the data.\n",
    "\n",
    "### Two Main Categories:\n",
    "\n",
    "| Category | Goal | Examples |\n",
    "|----------|------|----------|\n",
    "| **Clustering** | Group similar data points together | K-Means, DBSCAN, Hierarchical |\n",
    "| **Dimensionality Reduction** | Reduce the number of features while preserving important information | PCA, t-SNE, UMAP |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pca_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Principal Component Analysis (PCA)\n",
    "\n",
    "### 2.1 What is PCA?\n",
    "\n",
    "**PCA** is a dimensionality reduction technique that transforms data into a new coordinate system where:\n",
    "- The first axis (PC1) captures the **maximum variance**\n",
    "- Each subsequent axis captures the maximum remaining variance\n",
    "- All axes are **orthogonal** (perpendicular) to each other\n",
    "\n",
    "### 2.2 Mathematical Foundation\n",
    "\n",
    "#### Step 1: Center the Data\n",
    "Subtract the mean from each feature:\n",
    "$$\\bar{X} = X - \\mu$$\n",
    "\n",
    "where $\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i$\n",
    "\n",
    "#### Step 2: Compute Covariance Matrix\n",
    "$$C = \\frac{1}{n-1}\\bar{X}^T\\bar{X}$$\n",
    "\n",
    "#### Step 3: Eigenvalue Decomposition\n",
    "Find eigenvalues $\\lambda$ and eigenvectors $v$ such that:\n",
    "$$Cv = \\lambda v$$\n",
    "\n",
    "#### Step 4: Project Data\n",
    "Transform data using the top $k$ eigenvectors:\n",
    "$$Z = \\bar{X}W_k$$\n",
    "\n",
    "where $W_k$ contains the top $k$ eigenvectors as columns.\n",
    "\n",
    "#### Explained Variance Ratio\n",
    "$$\\text{Explained Variance Ratio}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{p}\\lambda_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data with multiple features\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset (4 features)\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "print(f\"Original shape: {X_iris.shape}\")\n",
    "print(f\"Features: {iris.feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Standardize the data (important for PCA!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=4)  # Keep all components first\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 3: Analyze explained variance\n",
    "print(\"Explained Variance Ratio per component:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.4f} ({var*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative explained variance:\")\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "for i, c in enumerate(cumsum):\n",
    "    print(f\"  PC1-{i+1}: {c:.4f} ({c*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot of individual explained variance\n",
    "ax1 = axes[0]\n",
    "components = [f'PC{i+1}' for i in range(len(pca.explained_variance_ratio_))]\n",
    "ax1.bar(components, pca.explained_variance_ratio_, color='steelblue', alpha=0.8)\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title('Explained Variance by Component')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Cumulative explained variance\n",
    "ax2 = axes[1]\n",
    "ax2.plot(components, cumsum, 'bo-', linewidth=2, markersize=8)\n",
    "ax2.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "ax2.fill_between(components, cumsum, alpha=0.3)\n",
    "ax2.set_xlabel('Principal Component')\n",
    "ax2.set_ylabel('Cumulative Explained Variance')\n",
    "ax2.set_title('Cumulative Explained Variance')\n",
    "ax2.set_ylim(0, 1.05)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_2d_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data in 2D PCA space\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], \n",
    "                      c=y_iris, cmap='viridis', \n",
    "                      edgecolors='white', s=100, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Species')\n",
    "plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "plt.title('Iris Dataset in 2D PCA Space')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total variance explained by 2 components: {sum(pca_2d.explained_variance_ratio_)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmeans_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. K-Means Clustering\n",
    "\n",
    "### 3.1 What is K-Means?\n",
    "\n",
    "**K-Means** is a partitioning algorithm that divides data into $k$ non-overlapping clusters. Each data point belongs to the cluster with the nearest centroid.\n",
    "\n",
    "### 3.2 Mathematical Foundation\n",
    "\n",
    "#### Objective Function (Inertia)\n",
    "K-Means minimizes the **Within-Cluster Sum of Squares (WCSS)**:\n",
    "\n",
    "$$J = \\sum_{i=1}^{n}\\sum_{j=1}^{k} r_{ij} \\|x_i - \\mu_j\\|^2$$\n",
    "\n",
    "where:\n",
    "- $r_{ij} = 1$ if point $x_i$ belongs to cluster $j$, otherwise $0$\n",
    "- $\\mu_j$ is the centroid of cluster $j$\n",
    "- $\\|x_i - \\mu_j\\|^2$ is the squared Euclidean distance\n",
    "\n",
    "#### Centroid Update\n",
    "$$\\mu_j = \\frac{\\sum_{i=1}^{n} r_{ij} x_i}{\\sum_{i=1}^{n} r_{ij}}$$\n",
    "\n",
    "### 3.3 Algorithm Steps\n",
    "\n",
    "1. **Initialize**: Randomly select $k$ initial centroids\n",
    "2. **Assign**: Assign each point to the nearest centroid\n",
    "3. **Update**: Recalculate centroids as mean of assigned points\n",
    "4. **Repeat**: Steps 2-3 until convergence (centroids don't change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmeans_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with clear cluster structure\n",
    "X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, \n",
    "                              cluster_std=0.60, random_state=42)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], s=50, alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Sample Data for Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmeans_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_blobs)\n",
    "\n",
    "# Get cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=cluster_labels, \n",
    "                      cmap='viridis', s=50, alpha=0.7)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', \n",
    "            s=300, edgecolors='black', linewidths=2, label='Centroids')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('K-Means Clustering Results (k=4)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Inertia (WCSS): {kmeans.inertia_:.2f}\")\n",
    "print(f\"Number of iterations: {kmeans.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elbow_section",
   "metadata": {},
   "source": [
    "### 3.4 Choosing the Optimal k: Elbow Method\n",
    "\n",
    "The **Elbow Method** helps determine the optimal number of clusters by plotting inertia vs. k and looking for the \"elbow\" point where the rate of decrease sharply changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elbow_method",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inertia for different values of k\n",
    "k_range = range(1, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_temp.fit(X_blobs)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    if k > 1:  # Silhouette score requires at least 2 clusters\n",
    "        silhouette_scores.append(silhouette_score(X_blobs, kmeans_temp.labels_))\n",
    "\n",
    "# Plot elbow curve and silhouette scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "ax1 = axes[0]\n",
    "ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia (WCSS)')\n",
    "ax1.set_title('Elbow Method')\n",
    "ax1.axvline(x=4, color='r', linestyle='--', label='Optimal k=4')\n",
    "ax1.legend()\n",
    "\n",
    "# Silhouette score plot\n",
    "ax2 = axes[1]\n",
    "ax2.plot(range(2, 11), silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Score vs Number of Clusters')\n",
    "ax2.axvline(x=4, color='r', linestyle='--', label='Optimal k=4')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbscan_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. DBSCAN (Density-Based Spatial Clustering)\n",
    "\n",
    "### 4.1 What is DBSCAN?\n",
    "\n",
    "**DBSCAN** groups together points that are closely packed together, marking points in low-density regions as outliers. Unlike K-Means, it:\n",
    "- Does **not** require specifying the number of clusters\n",
    "- Can find **arbitrarily shaped** clusters\n",
    "- Is robust to **outliers**\n",
    "\n",
    "### 4.2 Key Concepts\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| **ε (eps)** | Maximum distance between two points to be considered neighbors |\n",
    "| **MinPts** | Minimum number of points required to form a dense region |\n",
    "| **Core Point** | A point with at least MinPts neighbors within ε distance |\n",
    "| **Border Point** | A point within ε of a core point but with fewer than MinPts neighbors |\n",
    "| **Noise Point** | A point that is neither core nor border (outlier) |\n",
    "\n",
    "### 4.3 Mathematical Definition\n",
    "\n",
    "#### ε-neighborhood\n",
    "$$N_\\varepsilon(x) = \\{y \\in D : d(x, y) \\leq \\varepsilon\\}$$\n",
    "\n",
    "#### Core Point Condition\n",
    "$$|N_\\varepsilon(x)| \\geq \\text{MinPts}$$\n",
    "\n",
    "#### Density-Reachability\n",
    "A point $p$ is **directly density-reachable** from $q$ if:\n",
    "1. $p \\in N_\\varepsilon(q)$\n",
    "2. $q$ is a core point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbscan_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate moon-shaped data (non-convex clusters)\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# Load the moons.csv data\n",
    "X_csv = pd.read_csv('../data/moons.csv')\n",
    "y_csv = X_csv.pop('Label')\n",
    "\n",
    "# Compare K-Means vs DBSCAN on moon-shaped data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Original data\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis', s=50)\n",
    "ax1.set_title('Original Moon Data (True Labels)')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "\n",
    "# K-Means on moon data\n",
    "kmeans_moons = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans_moons.fit_predict(X_moons)\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(X_moons[:, 0], X_moons[:, 1], c=kmeans_labels, cmap='viridis', s=50)\n",
    "ax2.scatter(kmeans_moons.cluster_centers_[:, 0], kmeans_moons.cluster_centers_[:, 1], \n",
    "            c='red', marker='X', s=200, edgecolors='black')\n",
    "ax2.set_title('K-Means Clustering (Fails on Non-Convex Data)')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "\n",
    "# DBSCAN on moon data\n",
    "dbscan_moons = DBSCAN(eps=0.2, min_samples=5)\n",
    "dbscan_labels = dbscan_moons.fit_predict(X_moons)\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "# Handle noise points (-1 label)\n",
    "colors = dbscan_labels.copy().astype(float)\n",
    "colors[dbscan_labels == -1] = np.nan  # Noise points\n",
    "scatter = ax3.scatter(X_moons[:, 0], X_moons[:, 1], c=colors, cmap='viridis', s=50)\n",
    "ax3.scatter(X_moons[dbscan_labels == -1, 0], X_moons[dbscan_labels == -1, 1], \n",
    "            c='red', marker='x', s=50, label='Noise')\n",
    "ax3.set_title('DBSCAN Clustering (Handles Non-Convex Data)')\n",
    "ax3.set_xlabel('Feature 1')\n",
    "ax3.set_ylabel('Feature 2')\n",
    "ax3.legend()\n",
    "\n",
    "# DBSCAN on CSV data\n",
    "dbscan_csv = DBSCAN(eps=0.3, min_samples=5)\n",
    "dbscan_csv_labels = dbscan_csv.fit_predict(X_csv)\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(X_csv['Feature1'], X_csv['Feature2'], c=dbscan_csv_labels, cmap='viridis', s=50)\n",
    "ax4.set_title('DBSCAN on moons.csv Data')\n",
    "ax4.set_xlabel('Feature 1')\n",
    "ax4.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"DBSCAN found {len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)} clusters\")\n",
    "print(f\"Number of noise points: {sum(dbscan_labels == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Algorithm Comparison\n",
    "\n",
    "### 5.1 K-Means vs DBSCAN\n",
    "\n",
    "| Aspect | K-Means | DBSCAN |\n",
    "|--------|---------|--------|\n",
    "| **Cluster Shape** | Spherical (convex) | Arbitrary |\n",
    "| **Number of Clusters** | Must specify k | Automatically determined |\n",
    "| **Outlier Handling** | No (assigns all points) | Yes (marks as noise) |\n",
    "| **Scalability** | O(n×k×i) - Fast | O(n²) - Slower |\n",
    "| **Parameters** | k (number of clusters) | eps, min_samples |\n",
    "| **Sensitivity** | To initialization | To parameter choice |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create various datasets to compare algorithms\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Generate different data patterns\n",
    "datasets = [\n",
    "    ('Blobs', make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=42)),\n",
    "    ('Moons', make_moons(n_samples=300, noise=0.05, random_state=42)),\n",
    "    ('Circles', make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "for row, (name, (X, y)) in enumerate(datasets):\n",
    "    # Scale data\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Original\n",
    "    axes[row, 0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='viridis', s=30)\n",
    "    axes[row, 0].set_title(f'{name} - True Labels')\n",
    "    \n",
    "    # K-Means\n",
    "    km_labels = KMeans(n_clusters=len(set(y)), random_state=42, n_init=10).fit_predict(X_scaled)\n",
    "    axes[row, 1].scatter(X_scaled[:, 0], X_scaled[:, 1], c=km_labels, cmap='viridis', s=30)\n",
    "    axes[row, 1].set_title(f'{name} - K-Means')\n",
    "    \n",
    "    # DBSCAN\n",
    "    db_labels = DBSCAN(eps=0.3, min_samples=5).fit_predict(X_scaled)\n",
    "    axes[row, 2].scatter(X_scaled[:, 0], X_scaled[:, 1], c=db_labels, cmap='viridis', s=30)\n",
    "    axes[row, 2].set_title(f'{name} - DBSCAN')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Clustering Evaluation Metrics\n",
    "\n",
    "### 6.1 Internal Metrics (No Ground Truth Required)\n",
    "\n",
    "#### Silhouette Score\n",
    "Measures how similar a point is to its own cluster vs. other clusters:\n",
    "$$s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n",
    "\n",
    "where:\n",
    "- $a(i)$ = mean distance to other points in same cluster\n",
    "- $b(i)$ = mean distance to points in nearest different cluster\n",
    "- Range: [-1, 1], higher is better\n",
    "\n",
    "#### Calinski-Harabasz Index\n",
    "$$CH = \\frac{SS_B / (k-1)}{SS_W / (n-k)}$$\n",
    "\n",
    "where $SS_B$ is between-cluster variance and $SS_W$ is within-cluster variance.\n",
    "Higher values indicate better defined clusters.\n",
    "\n",
    "#### Davies-Bouldin Index\n",
    "$$DB = \\frac{1}{k}\\sum_{i=1}^{k} \\max_{j \\neq i} \\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)}$$\n",
    "\n",
    "Lower values indicate better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering on blobs data\n",
    "X_eval, y_eval = make_blobs(n_samples=500, centers=4, cluster_std=0.8, random_state=42)\n",
    "X_eval_scaled = StandardScaler().fit_transform(X_eval)\n",
    "\n",
    "# Compare different k values\n",
    "results = []\n",
    "for k in range(2, 8):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_eval_scaled)\n",
    "    \n",
    "    sil = silhouette_score(X_eval_scaled, labels)\n",
    "    ch = calinski_harabasz_score(X_eval_scaled, labels)\n",
    "    db = davies_bouldin_score(X_eval_scaled, labels)\n",
    "    \n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'Silhouette': sil,\n",
    "        'Calinski-Harabasz': ch,\n",
    "        'Davies-Bouldin': db,\n",
    "        'Inertia': kmeans.inertia_\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Clustering Evaluation Metrics:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Silhouette Score\n",
    "axes[0, 0].plot(results_df['k'], results_df['Silhouette'], 'bo-', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Number of Clusters')\n",
    "axes[0, 0].set_ylabel('Silhouette Score')\n",
    "axes[0, 0].set_title('Silhouette Score (Higher is Better)')\n",
    "axes[0, 0].axvline(x=4, color='r', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Calinski-Harabasz Index\n",
    "axes[0, 1].plot(results_df['k'], results_df['Calinski-Harabasz'], 'go-', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel('Number of Clusters')\n",
    "axes[0, 1].set_ylabel('Calinski-Harabasz Index')\n",
    "axes[0, 1].set_title('Calinski-Harabasz Index (Higher is Better)')\n",
    "axes[0, 1].axvline(x=4, color='r', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "axes[1, 0].plot(results_df['k'], results_df['Davies-Bouldin'], 'ro-', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Number of Clusters')\n",
    "axes[1, 0].set_ylabel('Davies-Bouldin Index')\n",
    "axes[1, 0].set_title('Davies-Bouldin Index (Lower is Better)')\n",
    "axes[1, 0].axvline(x=4, color='r', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Inertia\n",
    "axes[1, 1].plot(results_df['k'], results_df['Inertia'], 'mo-', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel('Number of Clusters')\n",
    "axes[1, 1].set_ylabel('Inertia (WCSS)')\n",
    "axes[1, 1].set_title('Inertia (Lower is Better, but watch for overfitting)')\n",
    "axes[1, 1].axvline(x=4, color='r', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **PCA** reduces dimensionality by projecting data onto principal components that capture maximum variance\n",
    "2. **K-Means** is fast and works well for spherical clusters, but requires specifying k\n",
    "3. **DBSCAN** can find arbitrary shaped clusters and handles outliers, but is sensitive to parameters\n",
    "4. **Always scale your data** before applying these algorithms\n",
    "5. Use **multiple metrics** to evaluate clustering quality\n",
    "\n",
    "### When to Use Each Algorithm\n",
    "\n",
    "| Situation | Recommended Algorithm |\n",
    "|-----------|----------------------|\n",
    "| High-dimensional data | Start with PCA |\n",
    "| Known number of clusters | K-Means |\n",
    "| Unknown number of clusters | DBSCAN |\n",
    "| Non-convex clusters | DBSCAN |\n",
    "| Large datasets | K-Means (faster) |\n",
    "| Outlier detection | DBSCAN |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Exercises\n",
    "\n",
    "### Exercise 1: PCA on House Prices Data\n",
    "Load the `house_prices.csv` data, apply PCA, and determine how many components are needed to explain 95% of the variance.\n",
    "\n",
    "### Exercise 2: Optimal K Selection\n",
    "Generate your own blob data with 5 centers and use the elbow method to find the optimal k. Verify with silhouette score.\n",
    "\n",
    "### Exercise 3: DBSCAN Parameter Tuning\n",
    "Experiment with different `eps` and `min_samples` values on the moons dataset. What happens when eps is too small? Too large?\n",
    "\n",
    "### Exercise 4: Algorithm Comparison\n",
    "Create a dataset with overlapping clusters. Compare K-Means and DBSCAN results. Which performs better?\n",
    "\n",
    "### Exercise 5: Real-World Application\n",
    "Use PCA to reduce the iris dataset to 2D, then apply both K-Means and DBSCAN. Compare results with actual species labels.\n",
    "\n",
    "### Exercise 6: Visualization Challenge\n",
    "Create a 3D visualization of the first 3 principal components of the iris dataset, colored by cluster assignments.\n",
    "\n",
    "### Exercise 7: Metrics Interpretation\n",
    "For a given clustering result, calculate and interpret all three internal metrics (Silhouette, Calinski-Harabasz, Davies-Bouldin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further_reading",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Further Reading\n",
    "\n",
    "- [scikit-learn Clustering Documentation](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "- [scikit-learn PCA Documentation](https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "- [Cluster Analysis Wikipedia](https://en.wikipedia.org/wiki/Cluster_analysis)\n",
    "- [DBSCAN Original Paper](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf)\n",
    "- [Silhouette Analysis](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}