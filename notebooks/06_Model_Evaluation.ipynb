{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc617d1f",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "## Week 6: Comprehensive Guide to Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objectives",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand** the importance of proper train/validation/test splits\n",
    "2. **Calculate and interpret** regression metrics (MAE, MSE, RMSE, R²)\n",
    "3. **Calculate and interpret** classification metrics (accuracy, precision, recall, F1)\n",
    "4. **Implement** cross-validation techniques\n",
    "5. **Plot and interpret** learning curves and validation curves\n",
    "6. **Diagnose** overfitting and underfitting using evaluation techniques\n",
    "7. **Use** confusion matrices and ROC curves for classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## 1. Introduction to Model Evaluation\n",
    "\n",
    "**Model evaluation** is crucial for:\n",
    "- Assessing how well a model generalizes to unseen data\n",
    "- Comparing different models\n",
    "- Tuning hyperparameters\n",
    "- Detecting overfitting/underfitting\n",
    "\n",
    "### The Fundamental Trade-off\n",
    "\n",
    "| Problem | Symptoms | Solutions |\n",
    "|---------|----------|------------|\n",
    "| **Underfitting** (High Bias) | Low train & test scores | More features, complex model |\n",
    "| **Overfitting** (High Variance) | High train, low test score | More data, regularization, simpler model |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, KFold, \n",
    "    learning_curve, validation_curve, StratifiedKFold\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, ConfusionMatrixDisplay, RocCurveDisplay\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification, load_iris\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Splitting Strategies\n",
    "\n",
    "### 2.1 Train/Validation/Test Split\n",
    "\n",
    "```\n",
    "        Full Dataset\n",
    "             |\n",
    "    +--------+--------+\n",
    "    |                 |\n",
    " Training        Test Set (15-20%)\n",
    "   Set               [Final evaluation only]\n",
    "    |\n",
    "    +--------+\n",
    "    |        |\n",
    " Train    Validation\n",
    "(60-70%)  (15-20%)\n",
    "           [Hyperparameter tuning]\n",
    "```\n",
    "\n",
    "### 2.2 Rationale\n",
    "\n",
    "| Split | Purpose | Usage |\n",
    "|-------|---------|-------|\n",
    "| **Training** | Learn model parameters | Fit the model |\n",
    "| **Validation** | Tune hyperparameters | Model selection |\n",
    "| **Test** | Final evaluation | Report performance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/house_prices.csv')\n",
    "X = df.drop('SalePrice', axis=1)\n",
    "y = df['SalePrice']\n",
    "\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "\n",
    "# Method 1: Simple train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"\\nSimple split:\")\n",
    "print(f\"  Training: {len(X_train)} ({len(X_train)/len(X)*100:.0f}%)\")\n",
    "print(f\"  Test: {len(X_test)} ({len(X_test)/len(X)*100:.0f}%)\")\n",
    "\n",
    "# Method 2: Train/validation/test split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=42  # 0.25 of 0.8 = 0.2\n",
    ")\n",
    "print(f\"\\nThree-way split:\")\n",
    "print(f\"  Training: {len(X_train)} ({len(X_train)/len(X)*100:.0f}%)\")\n",
    "print(f\"  Validation: {len(X_val)} ({len(X_val)/len(X)*100:.0f}%)\")\n",
    "print(f\"  Test: {len(X_test)} ({len(X_test)/len(X)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regression_metrics",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Regression Metrics\n",
    "\n",
    "### 3.1 Mean Absolute Error (MAE)\n",
    "\n",
    "Average of absolute differences between predictions and actual values:\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$\n",
    "\n",
    "**Interpretation**: Average error magnitude in original units.\n",
    "\n",
    "### 3.2 Mean Squared Error (MSE)\n",
    "\n",
    "Average of squared differences:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Interpretation**: Penalizes larger errors more than MAE.\n",
    "\n",
    "### 3.3 Root Mean Squared Error (RMSE)\n",
    "\n",
    "Square root of MSE:\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "**Interpretation**: Error in original units, but penalizes large errors.\n",
    "\n",
    "### 3.4 Coefficient of Determination (R²)\n",
    "\n",
    "Proportion of variance explained by the model:\n",
    "\n",
    "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$$\n",
    "\n",
    "where:\n",
    "- $SS_{res}$ = Residual sum of squares\n",
    "- $SS_{tot}$ = Total sum of squares\n",
    "- $\\bar{y}$ = Mean of actual values\n",
    "\n",
    "**Interpretation**: \n",
    "- R² = 1: Perfect predictions\n",
    "- R² = 0: Model predicts as well as the mean\n",
    "- R² < 0: Model is worse than predicting the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regression_metrics_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a regression model\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_val_pred = model.predict(X_val_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics for each set\n",
    "def calculate_regression_metrics(y_true, y_pred, set_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{set_name} Metrics:\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "    print(f\"  MSE:  {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  R²:   {r2:.4f}\")\n",
    "    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "train_metrics = calculate_regression_metrics(y_train, y_train_pred, \"Training\")\n",
    "val_metrics = calculate_regression_metrics(y_val, y_val_pred, \"Validation\")\n",
    "test_metrics = calculate_regression_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regression_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "datasets = [\n",
    "    (y_train, y_train_pred, 'Training'),\n",
    "    (y_val, y_val_pred, 'Validation'),\n",
    "    (y_test, y_test_pred, 'Test')\n",
    "]\n",
    "\n",
    "for ax, (y_true, y_pred, title) in zip(axes, datasets):\n",
    "    ax.scatter(y_true, y_pred, alpha=0.5, s=30)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax.set_xlabel('Actual Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    ax.set_title(f'{title} Set\\nR² = {r2_score(y_true, y_pred):.4f}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residuals_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(y_test_pred, residuals, alpha=0.5, s=30)\n",
    "ax1.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Predicted Values')\n",
    "ax1.set_ylabel('Residuals')\n",
    "ax1.set_title('Residuals vs Predicted Values')\n",
    "\n",
    "# Residual distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Residual')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title(f'Distribution of Residuals\\nMean: {residuals.mean():.4f}, Std: {residuals.std():.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classification_metrics",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Classification Metrics\n",
    "\n",
    "### 4.1 Confusion Matrix\n",
    "\n",
    "```\n",
    "                    Predicted\n",
    "                 Positive  Negative\n",
    "Actual Positive    TP        FN\n",
    "Actual Negative    FP        TN\n",
    "```\n",
    "\n",
    "- **TP** (True Positive): Correctly predicted positive\n",
    "- **TN** (True Negative): Correctly predicted negative\n",
    "- **FP** (False Positive): Incorrectly predicted positive (Type I Error)\n",
    "- **FN** (False Negative): Incorrectly predicted negative (Type II Error)\n",
    "\n",
    "### 4.2 Core Metrics\n",
    "\n",
    "#### Accuracy\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "**Warning**: Can be misleading with imbalanced classes!\n",
    "\n",
    "#### Precision (Positive Predictive Value)\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**Question it answers**: Of all positive predictions, how many are correct?\n",
    "\n",
    "#### Recall (Sensitivity, True Positive Rate)\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**Question it answers**: Of all actual positives, how many did we catch?\n",
    "\n",
    "#### F1 Score\n",
    "Harmonic mean of precision and recall:\n",
    "$$F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2TP}{2TP + FP + FN}$$\n",
    "\n",
    "### 4.3 When to Use Which Metric?\n",
    "\n",
    "| Scenario | Priority | Example |\n",
    "|----------|----------|----------|\n",
    "| Spam detection | Precision | Don't want legitimate emails in spam |\n",
    "| Disease detection | Recall | Don't want to miss sick patients |\n",
    "| Balanced importance | F1 Score | General classification tasks |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification data\n",
    "iris_df = pd.read_csv('../data/iris.csv')\n",
    "X_iris = iris_df.drop('class', axis=1)\n",
    "y_iris = iris_df['class']\n",
    "\n",
    "# Split data\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Predictions\n",
    "y_pred_c = clf.predict(X_test_c)\n",
    "y_prob_c = clf.predict_proba(X_test_c)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_c, y_pred_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion_matrix_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Without normalization\n",
    "cm = confusion_matrix(y_test_c, y_pred_c)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# With normalization\n",
    "cm_norm = confusion_matrix(y_test_c, y_pred_c, normalize='true')\n",
    "disp_norm = ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=clf.classes_)\n",
    "disp_norm.plot(ax=axes[1], cmap='Blues', values_format='.2f')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification example for ROC curve\n",
    "X_binary, y_binary = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=10,\n",
    "    n_redundant=5, n_classes=2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train multiple classifiers for comparison\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# ROC Curves\n",
    "ax1 = axes[0]\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_b, y_train_b)\n",
    "    y_prob = clf.predict_proba(X_test_b)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test_b, y_prob)\n",
    "    auc = roc_auc_score(y_test_b, y_prob)\n",
    "    ax1.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curves Comparison')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "# Precision-Recall Curves\n",
    "ax2 = axes[1]\n",
    "for name, clf in classifiers.items():\n",
    "    y_prob = clf.predict_proba(X_test_b)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test_b, y_prob)\n",
    "    ax2.plot(recall, precision, label=name, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curves')\n",
    "ax2.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cv_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Cross-Validation\n",
    "\n",
    "### 5.1 K-Fold Cross-Validation\n",
    "\n",
    "Divides data into k equal parts, uses k-1 for training and 1 for validation, rotating k times.\n",
    "\n",
    "```\n",
    "Fold 1: [Test] [Train] [Train] [Train] [Train]\n",
    "Fold 2: [Train] [Test] [Train] [Train] [Train]\n",
    "Fold 3: [Train] [Train] [Test] [Train] [Train]\n",
    "Fold 4: [Train] [Train] [Train] [Test] [Train]\n",
    "Fold 5: [Train] [Train] [Train] [Train] [Test]\n",
    "```\n",
    "\n",
    "### 5.2 Cross-Validation Score\n",
    "\n",
    "$$\\text{CV Score} = \\frac{1}{k}\\sum_{i=1}^{k} \\text{Score}_i$$\n",
    "\n",
    "### 5.3 Advantages\n",
    "- Uses all data for both training and validation\n",
    "- Provides a more robust estimate of model performance\n",
    "- Helps detect overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cv_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation demonstration\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "# Using our regression data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Simple cross-validation\n",
    "cv_scores = cross_val_score(LinearRegression(), X_scaled, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(f\"  Individual fold scores: {cv_scores}\")\n",
    "print(f\"  Mean R²: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "\n",
    "# Multiple metrics at once\n",
    "scoring = ['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error']\n",
    "cv_results = cross_validate(\n",
    "    LinearRegression(), X_scaled, y, cv=5, \n",
    "    scoring=scoring, return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\nCross-Validation with Multiple Metrics:\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['R²', 'MSE', 'MAE'],\n",
    "    'Train Mean': [\n",
    "        cv_results['train_r2'].mean(),\n",
    "        -cv_results['train_neg_mean_squared_error'].mean(),\n",
    "        -cv_results['train_neg_mean_absolute_error'].mean()\n",
    "    ],\n",
    "    'Test Mean': [\n",
    "        cv_results['test_r2'].mean(),\n",
    "        -cv_results['test_neg_mean_squared_error'].mean(),\n",
    "        -cv_results['test_neg_mean_absolute_error'].mean()\n",
    "    ],\n",
    "    'Test Std': [\n",
    "        cv_results['test_r2'].std(),\n",
    "        cv_results['test_neg_mean_squared_error'].std(),\n",
    "        cv_results['test_neg_mean_absolute_error'].std()\n",
    "    ]\n",
    "})\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cv_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create a visual representation of the folds\n",
    "n_samples = len(X)\n",
    "indices = np.arange(n_samples)\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "    # Create array for this fold (0 = train, 1 = validation)\n",
    "    fold_array = np.zeros(n_samples)\n",
    "    fold_array[val_idx] = 1\n",
    "    \n",
    "    ax.scatter(indices, [fold_idx] * n_samples, c=fold_array, \n",
    "               cmap='RdYlBu', s=10, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Fold')\n",
    "ax.set_title('K-Fold Cross-Validation Splits')\n",
    "ax.set_yticks(range(5))\n",
    "ax.set_yticklabels([f'Fold {i+1}' for i in range(5)])\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#4575b4', label='Training'),\n",
    "    Patch(facecolor='#d73027', label='Validation')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning_curves_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Learning Curves\n",
    "\n",
    "### 6.1 What are Learning Curves?\n",
    "\n",
    "Learning curves show how training and validation scores change as the **amount of training data** increases.\n",
    "\n",
    "### 6.2 Diagnosing Problems\n",
    "\n",
    "| Pattern | Diagnosis | Solution |\n",
    "|---------|-----------|----------|\n",
    "| Both scores low | **High Bias** (Underfitting) | More features, complex model |\n",
    "| Train high, Val low | **High Variance** (Overfitting) | More data, regularization |\n",
    "| Both converge high | **Good Fit** | Model is appropriate |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "learning_curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, X, y, title, ax=None, cv=5):\n",
    "    \"\"\"Plot learning curve for a given estimator.\"\"\"\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='r2'\n",
    "    )\n",
    "    \n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    train_std = train_scores.std(axis=1)\n",
    "    val_mean = val_scores.mean(axis=1)\n",
    "    val_std = val_scores.std(axis=1)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n",
    "                    alpha=0.1, color='blue')\n",
    "    ax.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, \n",
    "                    alpha=0.1, color='orange')\n",
    "    ax.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
    "    ax.plot(train_sizes, val_mean, 'o-', color='orange', label='Cross-Val Score')\n",
    "    \n",
    "    ax.set_xlabel('Training Set Size')\n",
    "    ax.set_ylabel('R² Score')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Compare different models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Simple model (potential underfitting)\n",
    "plot_learning_curve(\n",
    "    LinearRegression(), X_scaled, y,\n",
    "    'Linear Regression\\n(Potential High Bias)', axes[0]\n",
    ")\n",
    "\n",
    "# Moderate complexity\n",
    "plot_learning_curve(\n",
    "    Ridge(alpha=1.0), X_scaled, y,\n",
    "    'Ridge Regression\\n(Balanced)', axes[1]\n",
    ")\n",
    "\n",
    "# Complex model (potential overfitting)\n",
    "plot_learning_curve(\n",
    "    DecisionTreeRegressor(max_depth=None), X_scaled, y,\n",
    "    'Decision Tree (Unlimited Depth)\\n(Potential High Variance)', axes[2]\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation_curves_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Validation Curves\n",
    "\n",
    "### 7.1 What are Validation Curves?\n",
    "\n",
    "Validation curves show how training and validation scores change as a **hyperparameter** varies.\n",
    "\n",
    "### 7.2 Uses\n",
    "- Find optimal hyperparameter values\n",
    "- Detect underfitting/overfitting regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation_curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve for Decision Tree depth\n",
    "param_range = np.arange(1, 20)\n",
    "\n",
    "train_scores, val_scores = validation_curve(\n",
    "    DecisionTreeRegressor(random_state=42), X_scaled, y,\n",
    "    param_name='max_depth', param_range=param_range,\n",
    "    cv=5, scoring='r2'\n",
    ")\n",
    "\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "val_mean = val_scores.mean(axis=1)\n",
    "val_std = val_scores.std(axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.fill_between(param_range, train_mean - train_std, train_mean + train_std, \n",
    "                alpha=0.1, color='blue')\n",
    "ax.fill_between(param_range, val_mean - val_std, val_mean + val_std, \n",
    "                alpha=0.1, color='orange')\n",
    "ax.plot(param_range, train_mean, 'o-', color='blue', label='Training Score', linewidth=2)\n",
    "ax.plot(param_range, val_mean, 'o-', color='orange', label='Cross-Val Score', linewidth=2)\n",
    "\n",
    "# Find optimal depth\n",
    "optimal_depth = param_range[np.argmax(val_mean)]\n",
    "ax.axvline(x=optimal_depth, color='r', linestyle='--', label=f'Optimal Depth = {optimal_depth}')\n",
    "\n",
    "ax.set_xlabel('Max Depth')\n",
    "ax.set_ylabel('R² Score')\n",
    "ax.set_title('Validation Curve for Decision Tree')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True)\n",
    "\n",
    "# Annotate regions\n",
    "ax.annotate('Underfitting\\n(High Bias)', xy=(3, 0.4), fontsize=10, ha='center')\n",
    "ax.annotate('Overfitting\\n(High Variance)', xy=(15, 0.4), fontsize=10, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal max_depth: {optimal_depth}\")\n",
    "print(f\"Best CV Score: {val_mean[optimal_depth-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Data Splitting**: Always use separate test set for final evaluation\n",
    "2. **Regression Metrics**: Choose based on problem requirements (RMSE for penalizing large errors, MAE for robustness)\n",
    "3. **Classification Metrics**: Consider class imbalance; don't rely on accuracy alone\n",
    "4. **Cross-Validation**: Provides more robust performance estimates\n",
    "5. **Learning Curves**: Diagnose bias vs variance problems\n",
    "6. **Validation Curves**: Find optimal hyperparameters\n",
    "\n",
    "### Metric Selection Guide\n",
    "\n",
    "| Problem Type | Metric | When to Use |\n",
    "|--------------|--------|-------------|\n",
    "| Regression | RMSE | Penalize large errors |\n",
    "| Regression | MAE | Robust to outliers |\n",
    "| Regression | R² | Compare model explanatory power |\n",
    "| Classification | Precision | Minimize false positives |\n",
    "| Classification | Recall | Minimize false negatives |\n",
    "| Classification | F1 | Balance precision and recall |\n",
    "| Classification | AUC-ROC | Overall discrimination ability |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercises\n",
    "\n",
    "### Exercise 1: Regression Evaluation\n",
    "Train multiple regression models (Linear, Ridge, Lasso) on the house prices data. Compare them using MAE, RMSE, and R² on a held-out test set.\n",
    "\n",
    "### Exercise 2: Cross-Validation Comparison\n",
    "Compare 5-fold and 10-fold cross-validation results for a classifier. How do the variance of scores differ?\n",
    "\n",
    "### Exercise 3: Learning Curve Analysis\n",
    "Generate learning curves for a RandomForestClassifier on the iris dataset. Interpret what the curves tell you about bias and variance.\n",
    "\n",
    "### Exercise 4: ROC Curve Interpretation\n",
    "Create a binary classification problem and plot ROC curves for at least 3 different classifiers. Which one performs best?\n",
    "\n",
    "### Exercise 5: Hyperparameter Tuning\n",
    "Use validation curves to find the optimal `C` parameter for LogisticRegression on the iris dataset.\n",
    "\n",
    "### Exercise 6: Imbalanced Data\n",
    "Create an imbalanced dataset (90% one class, 10% other). Show why accuracy is misleading and which metrics are more appropriate.\n",
    "\n",
    "### Exercise 7: Stratified Sampling\n",
    "Compare regular train_test_split with stratified splitting for an imbalanced classification problem. How do the class distributions differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further_reading",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Further Reading\n",
    "\n",
    "- [scikit-learn Model Evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [scikit-learn Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Understanding ROC Curves](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "- [Precision-Recall Trade-off](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)\n",
    "- [Learning Curves Guide](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}