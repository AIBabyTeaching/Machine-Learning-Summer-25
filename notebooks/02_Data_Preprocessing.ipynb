{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6bc2a5",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff91e6b",
   "metadata": {},
   "source": [
    "Data preprocessing ensures raw data is transformed into a clean representation suitable for modeling. In this lab we demonstrate practical techniques to standardize features, handle missing entries, validate models using cross-validation, incorporate regularization, and compress features with PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9637d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "X,y=load_iris(return_X_y=True)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n",
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456427b",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Many algorithms assume features have comparable scales. Standardization rescales each feature to zero mean and unit variance using\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}.$$\n",
    "\n",
    "This prevents attributes with large magnitudes from dominating the learning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fe0f8",
   "metadata": {},
   "source": [
    "### Missing Value Handling\n",
    "\n",
    "Real-world datasets often contain missing values. Using `SimpleImputer` we can replace them with statistics such as the mean, median, or a constant placeholder. Proper imputation allows models that do not accept NaNs to be trained on imperfect data.\n",
    "\n",
    "$$\\hat{x}_{ij}=\\begin{cases}x_{ij}, & x_{ij}\\; \text{observed} \\\\ m_j, & x_{ij}\\; \text{missing}\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d375877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_missing = X_train.copy()\n",
    "X_missing.ravel()[::40] = np.nan\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X_imputed = imp.fit_transform(X_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb60c7",
   "metadata": {},
   "source": [
    "### Alternative Scaling\n",
    "\n",
    "Minâ€“max scaling linearly maps each feature to the [0, 1] interval. It preserves the shape of the distribution and is useful when features have known bounds or when we want to maintain sparsity. The `MinMaxScaler` from scikit-learn performs this transformation.\n",
    "\n",
    "$$x' = ({x - x_{\\min}})/({x_{\\max} - x_{\\min}})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec905f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "X_mm = mm.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78618e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_scaled=sc.fit_transform(X_train)\n",
    "X_scaled[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c18921",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Instead of relying on a single train/test split, k-fold cross validation repeatedly partitions the data and averages results to provide a more reliable estimate of generalization performance. The `cross_val_score` function automates this procedure.\n",
    "\n",
    "$$\\mathrm{CV}(f)=\\frac{1}{k}\\sum_{i=1}^k L\\big(f^{(-i)}, D_i\\big)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation example\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf=LogisticRegression(max_iter=200)\n",
    "cv_scores=cross_val_score(clf,X_scaled,y_train,cv=5)\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38569bbc",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Regularization discourages overly complex models by adding a penalty term to the loss function. **Ridge** uses an L2 penalty while **Lasso** uses an L1 penalty that can drive coefficients to zero. Tuning the strength of the penalty helps balance bias and variance.\n",
    "\n",
    "Ridge solves\\n\n",
    "$$\\min_w \\sum_i (y_i - w^T x_i)^2 + \\alpha \\|w\\|_2^2,$$\n",
    "while Lasso replaces the squared norm with \\|w\\|_1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge=Ridge(alpha=1.0)\n",
    "ridge.fit(X_scaled,y_train)\n",
    "ridge.score(sc.transform(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd1d6ea",
   "metadata": {},
   "source": [
    "### Dimension Reduction\n",
    "\n",
    "Principal Component Analysis (PCA) rotates the feature space to new orthogonal axes ordered by variance. By keeping only the first few components we obtain a compressed representation that often reveals important structure and speeds up downstream algorithms.\n",
    "\n",
    "The covariance matrix is\\n\n",
    "$$C=\\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{x})(x_i-\\bar{x})^T,$$\n",
    "and projecting onto the first $k$ eigenvectors $W_k$ gives\\n\n",
    "$$Z = X W_k.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a394906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA example\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=2)\n",
    "X_pca=pca.fit_transform(X_scaled)\n",
    "X_pca[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=y_train, cmap='viridis')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA Projection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b4c1c",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Evaluate `LogisticRegression` on the Iris data with and without standardization. Report accuracy using 5-fold cross validation.\n",
    "2. Compare `StandardScaler`, `MinMaxScaler`, and `RobustScaler` on a dataset containing outliers. Visualize the scaled feature distributions.\n",
    "3. Fit a `Ridge` model for several `alpha` values and plot training vs validation scores.\n",
    "4. Apply PCA to the scaled Iris features and plot the explained variance ratio. Experiment with different numbers of components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d828cd6",
   "metadata": {},
   "source": [
    "### Hints\n",
    "\n",
    "- Wrap preprocessing steps and the estimator in a `Pipeline` so that cross validation includes scaling.\n",
    "- Use `cross_val_score` or `GridSearchCV` for fair comparisons.\n",
    "- Access `pca.explained_variance_ratio_` to see how much variance each component captures.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
