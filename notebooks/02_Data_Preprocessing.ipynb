{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6bc2a5",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff91e6b",
   "metadata": {},
   "source": [
    "Data preprocessing ensures raw data is transformed into a clean representation suitable for modeling. In this lab we demonstrate practical techniques to standardize features, handle missing entries, validate models using cross-validation, incorporate regularization, and compress features with PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c43e3c",
   "metadata": {},
   "source": [
    "### Data Preprocessing Workflow\n",
    "```text\n",
    "Raw Data -> [Cleaning] -> [Encoding] -> [Scaling] -> Processed Data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9637d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "X,y=load_iris(return_X_y=True)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n",
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456427b",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Many algorithms assume features have comparable scales. Standardization rescales each feature to zero mean and unit variance using\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}.$$\n",
    "\n",
    "This prevents attributes with large magnitudes from dominating the learning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034312a",
   "metadata": {},
   "source": [
    "- $z$: standardized value\n",
    "- $x$: original feature value\n",
    "- $\\mu$: mean of the feature\n",
    "- $\\sigma$: standard deviation of the feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fe0f8",
   "metadata": {},
   "source": [
    "### Missing Value Handling\n",
    "\n",
    "Real-world datasets often contain missing values. Using `SimpleImputer` we can replace them with statistics such as the mean, median, or a constant placeholder. Proper imputation allows models that do not accept NaNs to be trained on imperfect data.\n",
    "\n",
    "$$\\hat{x}_{ij}=\\begin{cases}x_{ij}, & x_{ij}\\; \text{observed} \\\\ m_j, & x_{ij}\\; \text{missing}\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef15eecb",
   "metadata": {},
   "source": [
    "- $\\hat{x}_{ij}$: imputed value for row $i$, feature $j$\n",
    "- $x_{ij}$: observed data value\n",
    "- $m_j$: statistic (e.g., mean) used when $x_{ij}$ is missing\n",
    "- $i$: row index\n",
    "- $j$: feature index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d375877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_missing = X_train.copy()\n",
    "X_missing.ravel()[::40] = np.nan\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X_imputed = imp.fit_transform(X_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb60c7",
   "metadata": {},
   "source": [
    "### Alternative Scaling\n",
    "\n",
    "Minâ€“max scaling linearly maps each feature to the [0, 1] interval. It preserves the shape of the distribution and is useful when features have known bounds or when we want to maintain sparsity. The `MinMaxScaler` from scikit-learn performs this transformation.\n",
    "\n",
    "$$x' = ({x - x_{\\min}})/({x_{\\max} - x_{\\min}})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce6c31",
   "metadata": {},
   "source": [
    "- $x'$: scaled value\n",
    "- $x$: original value\n",
    "- $x_{\\min}$: minimum value of the feature\n",
    "- $x_{\\max}$: maximum value of the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec905f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "X_mm = mm.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78618e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_scaled=sc.fit_transform(X_train)\n",
    "X_scaled[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c18921",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Instead of relying on a single train/test split, k-fold cross validation repeatedly partitions the data and averages results to provide a more reliable estimate of generalization performance. The `cross_val_score` function automates this procedure.\n",
    "\n",
    "$$\\mathrm{CV}(f)=\\frac{1}{k}\\sum_{i=1}^k L\\big(f^{(-i)}, D_i\\big)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a2b2a5",
   "metadata": {},
   "source": [
    "- $\\mathrm{CV}(f)$: cross-validation estimate of model $f$\n",
    "- $k$: number of folds\n",
    "- $L$: loss function\n",
    "- $f^{(-i)}$: model trained on all folds except $i$\n",
    "- $D_i$: validation data from fold $i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation example\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf=LogisticRegression(max_iter=200)\n",
    "cv_scores=cross_val_score(clf,X_scaled,y_train,cv=5)\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38569bbc",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Regularization discourages overly complex models by adding a penalty term to the loss function. **Ridge** uses an L2 penalty while **Lasso** uses an L1 penalty that can drive coefficients to zero. Tuning the strength of the penalty helps balance bias and variance.\n",
    "\n",
    "Ridge solves\\n\n",
    "$$\\min_w \\sum_i (y_i - w^T x_i)^2 + \\alpha \\|w\\|_2^2,$$\n",
    "while Lasso replaces the squared norm with \\|w\\|_1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa2655",
   "metadata": {},
   "source": [
    "- $w$: weight vector\n",
    "- $y_i$: target value for sample $i$\n",
    "- $x_i$: feature vector for sample $i$\n",
    "- $\\alpha$: regularization strength\n",
    "- $\\|w\\|_2^2$: squared L2-norm of $w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge=Ridge(alpha=1.0)\n",
    "ridge.fit(X_scaled,y_train)\n",
    "ridge.score(sc.transform(X_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd1d6ea",
   "metadata": {},
   "source": [
    "### Dimension Reduction\n",
    "\n",
    "Principal Component Analysis (PCA) rotates the feature space to new orthogonal axes ordered by variance. By keeping only the first few components we obtain a compressed representation that often reveals important structure and speeds up downstream algorithms.\n",
    "\n",
    "The covariance matrix is\\n\n",
    "$$C=\\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{x})(x_i-\\bar{x})^T,$$\n",
    "and projecting onto the first $k$ eigenvectors $W_k$ gives\\n\n",
    "$$Z = X W_k.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dded3bd",
   "metadata": {},
   "source": [
    "- $C$: covariance matrix\n",
    "- $n$: number of samples\n",
    "- $x_i$: sample vector $i$\n",
    "- $\\bar{x}$: mean vector of all samples\n",
    "- $T$: transpose operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89899f",
   "metadata": {},
   "source": [
    "- $Z$: data projected onto principal components\n",
    "- $X$: centered data matrix\n",
    "- $W_k$: first $k$ eigenvectors (principal components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a394906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA example\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=2)\n",
    "X_pca=pca.fit_transform(X_scaled)\n",
    "X_pca[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=y_train, cmap='viridis')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA Projection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83d1ec",
   "metadata": {},
   "source": [
    "### Results and Interpretation\n",
    "The PCA visualization above illustrates how preprocessing condenses information while preserving class structure. Standardization, imputation, and scaling make feature ranges comparable, enabling clearer clustering in the reduced space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b4c1c",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Evaluate `LogisticRegression` on the Iris data with and without standardization. Report accuracy using 5-fold cross validation.\n",
    "2. Compare `StandardScaler`, `MinMaxScaler`, and `RobustScaler` on a dataset containing outliers. Visualize the scaled feature distributions.\n",
    "3. Fit a `Ridge` model for several `alpha` values and plot training vs validation scores.\n",
    "4. Apply PCA to the scaled Iris features and plot the explained variance ratio. Experiment with different numbers of components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d828cd6",
   "metadata": {},
   "source": [
    "### Hints\n",
    "\n",
    "- Wrap preprocessing steps and the estimator in a `Pipeline` so that cross validation includes scaling.\n",
    "- Use `cross_val_score` or `GridSearchCV` for fair comparisons.\n",
    "- Access `pca.explained_variance_ratio_` to see how much variance each component captures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20432d0",
   "metadata": {},
   "source": [
    "### Why Preprocessing Matters\n",
    "\n",
    "Real-world datasets often contain missing values, inconsistent units, and noisy measurements. Proper preprocessing improves model performance and training stability.\n",
    "\n",
    "- **Min-max scaling** rescales features to a fixed range $[0, 1]$:\n",
    "\n",
    "  $$x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$$\n",
    "\n",
    "- **Standardization** centers features and scales them to unit variance:\n",
    "\n",
    "  $$z = \\frac{x - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa83f954",
   "metadata": {},
   "source": [
    "### Worked Example: Standardizing Features\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = np.array([[1.0, 10.0], [2.0, 20.0], [3.0, 30.0]])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n",
    "```\n",
    "\n",
    "The resulting array has zero mean and unit variance for each feature, helping gradient-based models converge faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c93498",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "1. Use `SimpleImputer` to replace missing values in a dataset and compare model performance with and without imputation.\n",
    "2. Apply `MinMaxScaler` to a dataset with skewed features and plot histograms before and after scaling.\n",
    "3. Combine preprocessing steps in a `ColumnTransformer` that handles numeric and categorical data.\n",
    "4. Explore how `RobustScaler` affects models when outliers are present.\n",
    "5. Implement PCA to reduce the dataset to two components and visualize the result.\n",
    "6. Create a custom transformer that applies a log transform to skewed features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitocluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
