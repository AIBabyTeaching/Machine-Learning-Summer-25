{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29941e2",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "## Week 7: From Perceptrons to Multi-Layer Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objectives",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand** the biological inspiration behind neural networks\n",
    "2. **Explain** the structure and operation of a single perceptron\n",
    "3. **Identify** common activation functions and their properties\n",
    "4. **Build** a Multi-Layer Perceptron (MLP) using scikit-learn\n",
    "5. **Understand** the basics of forward and backward propagation\n",
    "6. **Visualize** decision boundaries of neural networks\n",
    "7. **Tune** hyperparameters for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 What is a Neural Network?\n",
    "\n",
    "A **Neural Network** is a computational model inspired by biological neurons in the brain. It consists of interconnected nodes (neurons) organized in layers.\n",
    "\n",
    "### 1.2 Brief History\n",
    "\n",
    "| Year | Milestone |\n",
    "|------|----------|\n",
    "| 1943 | McCulloch & Pitts propose artificial neuron |\n",
    "| 1958 | Rosenblatt introduces the Perceptron |\n",
    "| 1969 | Minsky & Papert show limitations (XOR problem) |\n",
    "| 1986 | Backpropagation popularized by Rumelhart et al. |\n",
    "| 2012+ | Deep Learning revolution (ImageNet, GPT, etc.) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, learning_curve, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceptron_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Perceptron\n",
    "\n",
    "### 2.1 Structure\n",
    "\n",
    "A **perceptron** is the simplest neural network unit:\n",
    "\n",
    "```\n",
    "         x₁ ----w₁----\\\n",
    "                        \\\n",
    "         x₂ ----w₂------>  Σ  ---> σ(z) ---> ŷ\n",
    "                        /\n",
    "         xₙ ----wₙ----/\n",
    "                  ↑\n",
    "                 +b (bias)\n",
    "```\n",
    "\n",
    "### 2.2 Mathematical Model\n",
    "\n",
    "#### Linear Combination (Weighted Sum)\n",
    "$$z = \\sum_{i=1}^{n} w_i x_i + b = w^T x + b$$\n",
    "\n",
    "where:\n",
    "- $x_i$ = input features\n",
    "- $w_i$ = weights (learnable parameters)\n",
    "- $b$ = bias term\n",
    "\n",
    "#### Activation Function\n",
    "$$\\hat{y} = \\sigma(z)$$\n",
    "\n",
    "The activation function $\\sigma$ introduces **non-linearity** into the model.\n",
    "\n",
    "### 2.3 Perceptron Learning Rule\n",
    "\n",
    "For binary classification:\n",
    "$$w_{i}^{(t+1)} = w_{i}^{(t)} + \\eta (y - \\hat{y}) x_i$$\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceptron_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple perceptron implementation for educational purposes\n",
    "class SimplePerceptron:\n",
    "    \"\"\"A simple perceptron for binary classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=100):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iter = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors_ = []  # Track errors during training\n",
    "    \n",
    "    def _step_function(self, z):\n",
    "        \"\"\"Step activation function.\"\"\"\n",
    "        return np.where(z >= 0, 1, 0)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, yi in zip(X, y):\n",
    "                # Forward pass\n",
    "                z = np.dot(xi, self.weights) + self.bias\n",
    "                y_pred = self._step_function(z)\n",
    "                \n",
    "                # Update weights\n",
    "                update = self.lr * (yi - y_pred)\n",
    "                self.weights += update * xi\n",
    "                self.bias += update\n",
    "                \n",
    "                errors += int(yi != y_pred)\n",
    "            \n",
    "            self.errors_.append(errors)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self._step_function(z)\n",
    "\n",
    "# Create linearly separable data\n",
    "from sklearn.datasets import make_blobs\n",
    "X_simple, y_simple = make_blobs(n_samples=100, centers=2, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Train perceptron\n",
    "perceptron = SimplePerceptron(learning_rate=0.01, n_iterations=50)\n",
    "perceptron.fit(X_simple, y_simple)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Decision boundary\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(X_simple[y_simple==0, 0], X_simple[y_simple==0, 1], label='Class 0', alpha=0.7, s=50)\n",
    "ax1.scatter(X_simple[y_simple==1, 0], X_simple[y_simple==1, 1], label='Class 1', alpha=0.7, s=50)\n",
    "\n",
    "# Plot decision boundary\n",
    "x_min, x_max = X_simple[:, 0].min() - 1, X_simple[:, 0].max() + 1\n",
    "y_boundary = -(perceptron.weights[0] * np.linspace(x_min, x_max, 100) + perceptron.bias) / perceptron.weights[1]\n",
    "ax1.plot(np.linspace(x_min, x_max, 100), y_boundary, 'r--', linewidth=2, label='Decision Boundary')\n",
    "\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_title('Perceptron Decision Boundary')\n",
    "ax1.legend()\n",
    "\n",
    "# Training errors\n",
    "ax2 = axes[1]\n",
    "ax2.plot(range(1, len(perceptron.errors_) + 1), perceptron.errors_, marker='o')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Number of Misclassifications')\n",
    "ax2.set_title('Perceptron Training Progress')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final weights: {perceptron.weights}\")\n",
    "print(f\"Final bias: {perceptron.bias}\")\n",
    "print(f\"Training accuracy: {accuracy_score(y_simple, perceptron.predict(X_simple))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activation_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Activation Functions\n",
    "\n",
    "### 3.1 Why Non-linear Activation?\n",
    "\n",
    "Without non-linear activation functions, a neural network with multiple layers would still be equivalent to a single linear transformation. Non-linearity allows networks to learn complex patterns.\n",
    "\n",
    "### 3.2 Common Activation Functions\n",
    "\n",
    "#### Sigmoid (Logistic)\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Properties**: Output range (0, 1), smooth gradient, but suffers from vanishing gradient.\n",
    "\n",
    "#### Hyperbolic Tangent (tanh)\n",
    "$$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "\n",
    "**Properties**: Output range (-1, 1), zero-centered, still has vanishing gradient.\n",
    "\n",
    "#### Rectified Linear Unit (ReLU)\n",
    "$$\\text{ReLU}(z) = \\max(0, z)$$\n",
    "\n",
    "**Properties**: Simple, avoids vanishing gradient for positive values, but \"dying ReLU\" problem.\n",
    "\n",
    "#### Leaky ReLU\n",
    "$$\\text{LeakyReLU}(z) = \\begin{cases} z & \\text{if } z > 0 \\\\ \\alpha z & \\text{if } z \\leq 0 \\end{cases}$$\n",
    "\n",
    "**Properties**: Addresses dying ReLU problem with small slope for negative values.\n",
    "\n",
    "#### Softmax (for multi-class output)\n",
    "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "\n",
    "**Properties**: Outputs sum to 1, used for probability distribution over classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activation_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def leaky_relu(z, alpha=0.1):\n",
    "    return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "# Derivatives\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Activation functions\n",
    "activations = [\n",
    "    ('Sigmoid', sigmoid, sigmoid_derivative),\n",
    "    ('Tanh', tanh, tanh_derivative),\n",
    "    ('ReLU', relu, relu_derivative),\n",
    "    ('Leaky ReLU', leaky_relu, lambda z: np.where(z > 0, 1, 0.1))\n",
    "]\n",
    "\n",
    "for idx, (name, func, deriv) in enumerate(activations):\n",
    "    # Function\n",
    "    axes[0, idx].plot(z, func(z), 'b-', linewidth=2)\n",
    "    axes[0, idx].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[0, idx].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[0, idx].set_title(f'{name}')\n",
    "    axes[0, idx].set_xlabel('z')\n",
    "    axes[0, idx].set_ylabel('σ(z)')\n",
    "    axes[0, idx].grid(True)\n",
    "    \n",
    "    # Derivative\n",
    "    axes[1, idx].plot(z, deriv(z), 'r-', linewidth=2)\n",
    "    axes[1, idx].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[1, idx].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[1, idx].set_title(f'{name} Derivative')\n",
    "    axes[1, idx].set_xlabel('z')\n",
    "    axes[1, idx].set_ylabel(\"σ'(z)\")\n",
    "    axes[1, idx].grid(True)\n",
    "\n",
    "axes[0, 0].set_ylabel('Activation σ(z)')\n",
    "axes[1, 0].set_ylabel(\"Derivative σ'(z)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlp_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Multi-Layer Perceptron (MLP)\n",
    "\n",
    "### 4.1 Architecture\n",
    "\n",
    "```\n",
    "Input Layer      Hidden Layer(s)      Output Layer\n",
    "                                        \n",
    "    x₁ --------O----------O----------O--- ŷ₁\n",
    "              / \\        / \\\n",
    "    x₂ ------O---O------O---O--------O--- ŷ₂\n",
    "              \\ /        \\ /\n",
    "    x₃ --------O----------O----------O--- ŷ₃\n",
    "```\n",
    "\n",
    "### 4.2 Forward Propagation\n",
    "\n",
    "For layer $l$:\n",
    "$$z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$$\n",
    "$$a^{[l]} = \\sigma(z^{[l]})$$\n",
    "\n",
    "where:\n",
    "- $a^{[l-1]}$ = activations from previous layer\n",
    "- $W^{[l]}$ = weight matrix for layer $l$\n",
    "- $b^{[l]}$ = bias vector for layer $l$\n",
    "\n",
    "### 4.3 Loss Functions\n",
    "\n",
    "#### Binary Cross-Entropy\n",
    "$$L = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$\n",
    "\n",
    "#### Categorical Cross-Entropy (Multi-class)\n",
    "$$L = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1}^{K} y_{i,k} \\log(\\hat{y}_{i,k})$$\n",
    "\n",
    "#### Mean Squared Error (Regression)\n",
    "$$L = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlp_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate MLP on non-linear data\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Compare Logistic Regression vs MLP\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'MLP (1 hidden layer, 10 units)': MLPClassifier(\n",
    "        hidden_layer_sizes=(10,), max_iter=1000, random_state=42\n",
    "    ),\n",
    "    'MLP (2 hidden layers, 10 units each)': MLPClassifier(\n",
    "        hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42\n",
    "    ),\n",
    "    'MLP (50 units)': MLPClassifier(\n",
    "        hidden_layer_sizes=(50,), max_iter=1000, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, (name, model) in zip(axes, models.items()):\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Create mesh grid for decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_train_scaled[:, 0].min() - 0.5, X_train_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_train_scaled[:, 1].min() - 0.5, X_train_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    ax.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, \n",
    "               cmap='RdBu', edgecolors='black', s=50, alpha=0.7)\n",
    "    \n",
    "    train_acc = model.score(X_train_scaled, y_train)\n",
    "    test_acc = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    ax.set_title(f'{name}\\nTrain: {train_acc:.3f}, Test: {test_acc:.3f}')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backprop_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Backpropagation\n",
    "\n",
    "### 5.1 The Concept\n",
    "\n",
    "**Backpropagation** is the algorithm used to train neural networks by computing gradients efficiently using the chain rule.\n",
    "\n",
    "### 5.2 Gradient Descent Update\n",
    "\n",
    "For each parameter:\n",
    "$$\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\frac{\\partial L}{\\partial \\theta}$$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "### 5.3 Chain Rule Application\n",
    "\n",
    "For a weight $w$ in layer $l$:\n",
    "$$\\frac{\\partial L}{\\partial w^{[l]}} = \\frac{\\partial L}{\\partial a^{[L]}} \\cdot \\frac{\\partial a^{[L]}}{\\partial z^{[L]}} \\cdot \\frac{\\partial z^{[L]}}{\\partial a^{[L-1]}} \\cdots \\frac{\\partial z^{[l]}}{\\partial w^{[l]}}$$\n",
    "\n",
    "### 5.4 Gradient Descent Variants\n",
    "\n",
    "| Variant | Description |\n",
    "|---------|-------------|\n",
    "| **Batch GD** | Uses all samples for each update (slow) |\n",
    "| **Stochastic GD** | Uses one sample per update (noisy) |\n",
    "| **Mini-batch GD** | Uses small batches (best of both) |\n",
    "| **Adam** | Adaptive learning rate with momentum |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(50, 30), \n",
    "    max_iter=500, \n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    early_stopping=False\n",
    ")\n",
    "\n",
    "# Custom training to track loss\n",
    "losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "# Train incrementally\n",
    "mlp.partial_fit(X_train_scaled, y_train, classes=[0, 1])\n",
    "\n",
    "for epoch in range(200):\n",
    "    mlp.partial_fit(X_train_scaled, y_train)\n",
    "    losses.append(mlp.loss_)\n",
    "    train_accs.append(mlp.score(X_train_scaled, y_train))\n",
    "    test_accs.append(mlp.score(X_test_scaled, y_test))\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(losses, linewidth=2, color='blue')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss Over Time')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2 = axes[1]\n",
    "ax2.plot(train_accs, label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(test_accs, label='Test Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accs[-1]:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyperparameters_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Key Hyperparameters\n",
    "\n",
    "### 6.1 Network Architecture\n",
    "\n",
    "| Parameter | Description | Effect |\n",
    "|-----------|-------------|--------|\n",
    "| **Number of layers** | Depth of network | More layers = more complex patterns |\n",
    "| **Units per layer** | Width of each layer | More units = more capacity |\n",
    "\n",
    "### 6.2 Training Parameters\n",
    "\n",
    "| Parameter | Description | Typical Range |\n",
    "|-----------|-------------|---------------|\n",
    "| **Learning rate** | Step size for gradient descent | 0.0001 - 0.1 |\n",
    "| **Batch size** | Samples per gradient update | 32 - 256 |\n",
    "| **Epochs** | Number of passes through data | 10 - 1000+ |\n",
    "\n",
    "### 6.3 Regularization\n",
    "\n",
    "| Technique | Purpose |\n",
    "|-----------|----------|\n",
    "| **L2 (alpha)** | Penalizes large weights |\n",
    "| **Dropout** | Randomly disables neurons during training |\n",
    "| **Early stopping** | Stop training when validation score stops improving |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameter_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of hidden layer sizes\n",
    "architectures = [\n",
    "    (5,),\n",
    "    (10,),\n",
    "    (50,),\n",
    "    (100,),\n",
    "    (50, 30),\n",
    "    (100, 50, 25)\n",
    "]\n",
    "\n",
    "results = []\n",
    "for arch in architectures:\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=arch,\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    results.append({\n",
    "        'Architecture': str(arch),\n",
    "        'Train Acc': mlp.score(X_train_scaled, y_train),\n",
    "        'Test Acc': mlp.score(X_test_scaled, y_test),\n",
    "        'n_iter': mlp.n_iter_\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Architecture Comparison:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regularization_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of regularization (alpha)\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(alphas), figsize=(20, 4))\n",
    "\n",
    "for ax, alpha in zip(axes, alphas):\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(50,),\n",
    "        alpha=alpha,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_train_scaled[:, 0].min() - 0.5, X_train_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_train_scaled[:, 1].min() - 0.5, X_train_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    ax.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train,\n",
    "               cmap='RdBu', edgecolors='black', s=30)\n",
    "    \n",
    "    test_acc = mlp.score(X_test_scaled, y_test)\n",
    "    ax.set_title(f'alpha={alpha}\\nTest Acc: {test_acc:.3f}')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "plt.suptitle('Effect of L2 Regularization (alpha)', y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Practical Example: Iris Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iris_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data\n",
    "iris_df = pd.read_csv('../data/iris.csv')\n",
    "X_iris = iris_df.drop('class', axis=1)\n",
    "y_iris = iris_df['class']\n",
    "\n",
    "# Split and scale\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "scaler_iris = StandardScaler()\n",
    "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
    "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
    "\n",
    "# Train MLP\n",
    "mlp_iris = MLPClassifier(\n",
    "    hidden_layer_sizes=(50, 30),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "mlp_iris.fit(X_train_iris_scaled, y_train_iris)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_iris = mlp_iris.predict(X_test_iris_scaled)\n",
    "\n",
    "print(\"MLP Classification Report for Iris Dataset:\")\n",
    "print(classification_report(y_test_iris, y_pred_iris))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test_iris, y_pred_iris)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=mlp_iris.classes_, yticklabels=mlp_iris.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - MLP on Iris Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pros_cons_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Advantages and Disadvantages\n",
    "\n",
    "### Advantages\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Universal Approximation** | Can learn any continuous function |\n",
    "| **Feature Learning** | Automatically learns useful representations |\n",
    "| **Flexible Architecture** | Can be adapted to many problem types |\n",
    "| **Non-linear Patterns** | Handles complex, non-linear relationships |\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "| Disadvantage | Mitigation |\n",
    "|--------------|------------|\n",
    "| **Requires large data** | Use transfer learning, data augmentation |\n",
    "| **Computationally expensive** | Use GPUs, efficient architectures |\n",
    "| **Black box** | Use explainability techniques (SHAP, LIME) |\n",
    "| **Sensitive to hyperparameters** | Use automated hyperparameter tuning |\n",
    "| **Prone to overfitting** | Regularization, dropout, early stopping |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Perceptrons** are the basic building blocks of neural networks\n",
    "2. **Activation functions** introduce non-linearity (ReLU is most common)\n",
    "3. **MLPs** stack multiple layers to learn complex patterns\n",
    "4. **Backpropagation** efficiently computes gradients for training\n",
    "5. **Hyperparameter tuning** is crucial for good performance\n",
    "6. **Regularization** helps prevent overfitting\n",
    "\n",
    "### When to Use Neural Networks\n",
    "\n",
    "| Use Neural Networks When | Use Simpler Models When |\n",
    "|--------------------------|------------------------|\n",
    "| Large datasets available | Small datasets |\n",
    "| Complex non-linear patterns | Linear relationships |\n",
    "| Feature engineering is difficult | Clear feature engineering possible |\n",
    "| Prediction accuracy is priority | Interpretability is priority |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Exercises\n",
    "\n",
    "### Exercise 1: XOR Problem\n",
    "Create the XOR dataset (4 points: (0,0)->0, (0,1)->1, (1,0)->1, (1,1)->0) and show that a single perceptron cannot solve it, but an MLP can.\n",
    "\n",
    "### Exercise 2: Architecture Exploration\n",
    "Experiment with different architectures (number of layers, units per layer) on the moons dataset. Find the simplest architecture that achieves >95% test accuracy.\n",
    "\n",
    "### Exercise 3: Activation Function Comparison\n",
    "Train the same MLP architecture with different activation functions ('relu', 'tanh', 'logistic') and compare their training curves and final performance.\n",
    "\n",
    "### Exercise 4: Learning Rate Effect\n",
    "Train MLPs with different learning rates (0.0001, 0.001, 0.01, 0.1) and plot their loss curves. What happens when the learning rate is too high or too low?\n",
    "\n",
    "### Exercise 5: Regularization Study\n",
    "Create a small dataset that overfits easily. Show how increasing the alpha parameter (L2 regularization) reduces overfitting.\n",
    "\n",
    "### Exercise 6: Early Stopping\n",
    "Implement early stopping and compare the final test accuracy with and without it. How many epochs does early stopping save?\n",
    "\n",
    "### Exercise 7: Regression with MLP\n",
    "Use MLPRegressor on the house_prices.csv data. Compare its performance with Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further_reading",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Further Reading\n",
    "\n",
    "- [scikit-learn Neural Networks](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    "- [Deep Learning Book by Goodfellow et al.](https://www.deeplearningbook.org/) (Free online)\n",
    "- [3Blue1Brown Neural Networks Playlist](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) (Visual explanations)\n",
    "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) (Free online book)\n",
    "- [PyTorch Tutorials](https://pytorch.org/tutorials/) (For deeper neural network frameworks)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
